{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load .env file\n",
    "_ = load_dotenv(override=True)\n",
    "OPENAI_API_KEY = os.getenv('OPENAI_API_KEY')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* `Loading Documents`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain.document_loaders import PyPDFLoader, Docx2txtLoader, TextLoader, WikipediaLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "import tiktoken\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from langchain.chains import RetrievalQA, ConversationalRetrievalChain\n",
    "from langchain.vectorstores import Chroma \n",
    "from langchain.memory import ConversationBufferMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading PDF, DOCX and TXT files as LangChain Documents\n",
    "def load_document(file):\n",
    "    try:\n",
    "        # Get the extension of the attached file\n",
    "        _, extension = os.path.splitext(file)\n",
    "\n",
    "        if extension == '.pdf':\n",
    "            print(f'Loading {file}')\n",
    "            loader = PyPDFLoader(file)\n",
    "        elif extension == '.docx':\n",
    "            print(f'Loading {file}')\n",
    "            loader = Docx2txtLoader(file)\n",
    "        elif extension == '.txt':\n",
    "            loader = TextLoader(file)\n",
    "        else:\n",
    "            print('Document format is not supported!')\n",
    "            return None\n",
    "\n",
    "        # Load the file\n",
    "        data = loader.load()\n",
    "        return data\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: The file '{file}' was not found.\")\n",
    "        return None\n",
    "    except PermissionError:\n",
    "        print(f\"Error: Permission denied when accessing '{file}'.\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading document: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "# Wikipedia\n",
    "def load_from_wikipedia(query, lang='en', load_max_docs=2):\n",
    "    try:\n",
    "        # Wiki Loader\n",
    "        loader = WikipediaLoader(query=query, lang=lang, load_max_docs=load_max_docs)\n",
    "        data = loader.load()\n",
    "        return data\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading from Wikipedia: {str(e)}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* `Chunking`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_data(data, chunk_size=512, chunk_overlap=50):\n",
    "    # Used RecursiveCharacterTextSplitter with overlap\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size, \n",
    "        chunk_overlap=chunk_overlap,\n",
    "        length_function=len\n",
    "    )\n",
    "    chunks = text_splitter.split_documents(data)\n",
    "    print(f\"Created {len(chunks)} chunks from {len(data)} documents\")\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* `Calculating Cost for OpenAI Embeddings`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embeddings_cost(texts):\n",
    "    try:\n",
    "        # Using embedding model tokenizer\n",
    "        enc = tiktoken.encoding_for_model('text-embedding-3-small')\n",
    "        total_tokens = sum([len(enc.encode(page.page_content)) for page in texts])\n",
    "        cost_usd = (total_tokens * 0.020 / 1000000)\n",
    "        \n",
    "        print(f'Total Tokens: {total_tokens}')\n",
    "        print(f'Embedding Cost in USD: ${cost_usd:.6f}')\n",
    "        \n",
    "        return {\n",
    "            \"total_tokens\": total_tokens,\n",
    "            \"cost_usd\": cost_usd\n",
    "        }\n",
    "    except Exception as e:\n",
    "        print(f\"Error calculating embeddings cost: {str(e)}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  `Using ChromaDB as Vector Store`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_embeddings_chroma(chunks, persist_directory='../assets//chroma_db'):\n",
    "    try:\n",
    "        # embeddings model\n",
    "        embeddings = OpenAIEmbeddings(model='text-embedding-3-small', dimensions=1536)\n",
    "        \n",
    "        # Vector store using chroma\n",
    "        vector_store = Chroma.from_documents(\n",
    "            chunks, \n",
    "            embeddings, \n",
    "            persist_directory=persist_directory\n",
    "        )\n",
    "        \n",
    "        # Explicitly persist the vector store\n",
    "        vector_store.persist()\n",
    "        print(f\"Embeddings successfully created and saved to {persist_directory}\")\n",
    "        \n",
    "        return vector_store\n",
    "    except Exception as e:\n",
    "        print(f\"Error generating embeddings: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "def load_embeddings_chroma(persist_directory='./chroma_db'):\n",
    "    try:\n",
    "        # embeddings model\n",
    "        embeddings = OpenAIEmbeddings(model='text-embedding-3-small', dimensions=1536)\n",
    "        \n",
    "        # Vector store using chroma    \n",
    "        vector_store = Chroma(\n",
    "            persist_directory=persist_directory, \n",
    "            embedding_function=embeddings\n",
    "        )\n",
    "        \n",
    "        print(f\"Embeddings successfully loaded from {persist_directory}\")\n",
    "        return vector_store\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading embeddings: {str(e)}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `Question Answering`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def questions_answering(vector_store, question: str, k=5, temperature=0.3):\n",
    "    try:\n",
    "        # Define a custom prompt template for better responses\n",
    "        template = \"\"\"\n",
    "        Answer the question based only on the following context:\n",
    "        {context}\n",
    "        \n",
    "        Question: {question}\n",
    "        \n",
    "        Answer:\n",
    "        \"\"\"\n",
    "        QA_CHAIN_PROMPT = PromptTemplate.from_template(template)\n",
    "        \n",
    "        # LLM with lower temperature for more factual answers\n",
    "        llm = ChatOpenAI(model='gpt-4o-mini', temperature=temperature)\n",
    "        \n",
    "        # Retriever\n",
    "        retriever = vector_store.as_retriever(\n",
    "            search_type='similarity', \n",
    "            search_kwargs={'k': k}\n",
    "        )\n",
    "        \n",
    "        # Chain \n",
    "        chain = RetrievalQA.from_chain_type(\n",
    "            llm=llm,\n",
    "            chain_type=\"stuff\", \n",
    "            retriever=retriever,\n",
    "            chain_type_kwargs={\"prompt\": QA_CHAIN_PROMPT}\n",
    "        )\n",
    "        \n",
    "        # Invoke question into the chain\n",
    "        answer = chain.invoke(question)\n",
    "        return answer\n",
    "    except Exception as e:\n",
    "        print(f\"Error in question answering: {str(e)}\")\n",
    "        return {\"result\": f\"Error: {str(e)}\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* `Testing`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1. Loading document...\n",
      "Loading ../assets/attention_is_all_you_need.pdf\n",
      "Successfully loaded 15 document sections\n",
      "\n",
      "2. Chunking document...\n",
      "Created 91 chunks from 15 documents\n",
      "Successfully created 91 chunks\n",
      "\n",
      "3. Calculating embedding costs...\n",
      "Total Tokens: 10479\n",
      "Embedding Cost in USD: $0.000210\n",
      "\n",
      "4. Generating embeddings and storing in Chroma...\n",
      "Embeddings successfully created and saved to ../assets//chroma_db\n",
      "Vector store successfully created\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\moham\\AppData\\Local\\Temp\\ipykernel_51136\\3760534641.py:14: LangChainDeprecationWarning: Since Chroma 0.4.x the manual persistence method is no longer supported as docs are automatically persisted.\n",
      "  vector_store.persist()\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    # Load the PDF file\n",
    "    print(\"\\n1. Loading document...\")\n",
    "    data = load_document('../assets/attention_is_all_you_need.pdf')\n",
    "    if not data:\n",
    "        print(\"Failed to load document. Exiting test.\")\n",
    "        exit()\n",
    "    print(f\"Successfully loaded {len(data)} document sections\")\n",
    "\n",
    "    # Chunking the file\n",
    "    print(\"\\n2. Chunking document...\")\n",
    "    chunks = chunk_data(data, chunk_size=512, chunk_overlap=50)\n",
    "    print(f\"Successfully created {len(chunks)} chunks\")\n",
    "\n",
    "    # Optional: Calculate embedding costs\n",
    "    print(\"\\n3. Calculating embedding costs...\")\n",
    "    cost_info = get_embeddings_cost(chunks)\n",
    "    \n",
    "    # Create embeddings using chroma\n",
    "    print(\"\\n4. Generating embeddings and storing in Chroma...\")\n",
    "    vector_store = generate_embeddings_chroma(chunks=chunks)\n",
    "    if not vector_store:\n",
    "        print(\"Failed to create vector store. Exiting test.\")\n",
    "        exit()\n",
    "    print(\"Vector store successfully created\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"\\nError during testing: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: For how long did the model had been trained?\n",
      "\n",
      "The model had been trained for a total of 300,000 steps, which took 3.5 days.\n"
     ]
    }
   ],
   "source": [
    "# Asking questions\n",
    "question = 'For how long did the model had been trained?'\n",
    "print(f\"Question: {question}\")\n",
    "print()\n",
    "answer = questions_answering(vector_store=vector_store, question=question, temperature=1.5)\n",
    "print(answer['result'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: Multiply that number by 2.\n",
      "\n",
      "The number mentioned in the context is not specified, so I cannot provide a specific answer to the question. If you provide a number, I can multiply it by 2 for you.\n"
     ]
    }
   ],
   "source": [
    "# We can't ask follow-up questions. There is no memory (chat history) available.\n",
    "question = 'Multiply that number by 2.'\n",
    "print(f\"Question: {question}\")\n",
    "print()\n",
    "answer = questions_answering(vector_store=vector_store, question=question, temperature=0.2)\n",
    "print(answer['result'])  # As expected"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* `Adding Memory`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conversation_answering(vector_store, question: str, k=5, temperature=0.3, conversation=None):\n",
    "    try:\n",
    "        # Create conversation chain if not already created\n",
    "        if conversation is None:\n",
    "            # Define custom prompt template for better responses\n",
    "            QA_PROMPT = PromptTemplate.from_template(\"\"\"\n",
    "                You are an expert AI assistant helping with questions about the paper \"Attention Is All You Need\".\n",
    "                Answer the question based only on the following context from the paper:\n",
    "\n",
    "                {context}\n",
    "\n",
    "                Question: {question}\n",
    "\n",
    "                Provide a precise, technical answer with specific details from the paper.\n",
    "                If the information is not in the context, state \"The paper doesn't provide this specific information in the retrieved sections.\"\n",
    "                \"\"\"\n",
    "            )\n",
    "                            \n",
    "            # LLM with appropriate temperature\n",
    "            llm = ChatOpenAI(model='gpt-4o-mini', temperature=temperature)\n",
    "            \n",
    "            # Retriever\n",
    "            retriever = vector_store.as_retriever(\n",
    "                search_type='similarity', \n",
    "                search_kwargs={'k': k}\n",
    "            )\n",
    "            \n",
    "            # Memory\n",
    "            memory = ConversationBufferMemory(\n",
    "                memory_key='chat_history',\n",
    "                output_key='answer',\n",
    "                return_messages=True\n",
    "            )\n",
    "            \n",
    "            # Conversational chain with custom prompt\n",
    "            chain = ConversationalRetrievalChain.from_llm(\n",
    "                llm=llm,\n",
    "                retriever=retriever,\n",
    "                memory=memory,\n",
    "                chain_type=\"stuff\",\n",
    "                return_source_documents=True,\n",
    "                combine_docs_chain_kwargs={\"prompt\": QA_PROMPT}\n",
    "            )\n",
    "            \n",
    "            conversation = {\n",
    "                \"chain\": chain,\n",
    "                \"memory\": memory\n",
    "            }\n",
    "        \n",
    "        # Invoke question into the chain\n",
    "        answer = conversation[\"chain\"].invoke({'question': question})\n",
    "        return answer, conversation\n",
    "    except Exception as e:\n",
    "        print(f\"Error in conversation answering: {str(e)}\")\n",
    "        return {\"answer\": f\"Error: {str(e)}\"}, conversation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: For how long did the model had been trained?\n",
      "The base models were trained for a total of 100,000 steps or 12 hours. The big models were trained for 300,000 steps, which took 3.5 days.\n",
      "----------------------------------------\n",
      "Question: Multiply the previous number by 2.\n",
      "The total training time for the big models is 3.5 days. To convert this into hours, we multiply by 24 hours/day:\n",
      "\n",
      "3.5 days * 24 hours/day = 84 hours.\n",
      "\n",
      "Now, multiplying the total training time by 2:\n",
      "\n",
      "84 hours * 2 = 168 hours.\n",
      "\n",
      "Therefore, the result of multiplying the total training time of the big models by 2 is 168 hours.\n",
      "----------------------------------------\n",
      "Question: Positional encoding is added to the input embeddings. What is the purpose of this?\n",
      "The purpose of adding positional encoding to the input embeddings is to enable the model to make use of the position information in the sequence, as the model contains no recurrence and no convolution. This allows the model to attend to the position in the decoder to attend over all positions in the input sequence, mimicking typical encoder-decoder attention mechanisms in sequence-to-sequence models. The positional encodings have the same dimension as the embeddings, allowing them to be summed with the input embeddings. In this work, sine and cosine functions of different frequencies are used for the positional encodings, defined as:\n",
      "\n",
      "- \\( PE(pos, 2i) = \\sin\\left(\\frac{pos}{10000^{2i/d_{model}}}\\right) \\)\n",
      "- \\( PE(pos, 2i+1) = \\cos\\left(\\frac{pos}{10000^{2i/d_{model}}}\\right) \\)\n",
      "\n",
      "This encoding helps the model to incorporate the order of the tokens in the sequence.\n"
     ]
    }
   ],
   "source": [
    "# Initialize conversation\n",
    "conversation = None\n",
    "\n",
    "# First question\n",
    "question = 'For how long did the model had been trained?'\n",
    "print(f\"Question: {question}\")\n",
    "answer, conversation = conversation_answering(vector_store=vector_store, question=question, temperature=0.1, conversation=conversation)\n",
    "print(answer['answer'])\n",
    "print(\"----\"*10)\n",
    "\n",
    "# Follow-up question (now works with memory)\n",
    "question = 'Multiply the previous number by 2.'\n",
    "print(f\"Question: {question}\")\n",
    "answer, conversation = conversation_answering(vector_store=vector_store, question=question, temperature=0.1, conversation=conversation)\n",
    "print(answer['answer'])\n",
    "print(\"----\"*10)\n",
    "\n",
    "# Another follow-up\n",
    "question = 'Positional encoding is added to the input embeddings. What is the purpose of this?'\n",
    "print(f\"Question: {question}\")\n",
    "answer, conversation = conversation_answering(vector_store=vector_store, question=question, temperature=0.1, conversation=conversation)\n",
    "print(answer['answer'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "depi2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
